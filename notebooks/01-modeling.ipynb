{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ù§Ô∏è –ó“Ø—Ä—Ö–Ω–∏–π ”©–≤—á–Ω–∏–π —Ç–∞–∞–º–∞–≥–ª–∞–ª\n",
    "–≠–Ω—ç notebook –Ω—å –∑“Ø—Ä—Ö–Ω–∏–π ”©–≤—á–Ω–∏–π —Ç–∞–∞–º–∞–≥–ª–∞–ª—ã–≥ —Ö–∏–π—Ö –±“Ø—Ä—ç–Ω –º–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω pipeline-–∏–π–≥ –∞–≥—É—É–ª—Å–∞–Ω –±–æ–ª–Ω–æ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ –°–∞–Ω–≥—É—É–¥—ã–≥ –∏–º–ø–æ—Ä—Ç–ª–æ—Ö\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, RocCurveDisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf84d76",
   "metadata": {},
   "source": [
    "# ‚ù§Ô∏è –¢”©—Å”©–ª: –ó“Ø—Ä—Ö–Ω–∏–π ”®–≤—á–Ω–∏–π –¢–∞–∞–º–∞–≥–ª–∞–ª\n",
    "\n",
    "## üéØ –ó–æ—Ä–∏–ª–≥–æ  \n",
    "”®–≥”©–≥–¥—Å”©–Ω ”©–≤—á—Ç”©–Ω–∏–π –æ–Ω–æ—à–∏–ª–≥–æ–æ–Ω—ã –º—ç–¥—ç—ç–ª—ç–ª–¥ “Ø–Ω–¥—ç—Å–ª—ç–Ω —Ç—É—Ö–∞–π–Ω —Ö“Ø–Ω–∏–π –∑“Ø—Ä—Ö–Ω–∏–π ”©–≤—á—Ç—ç–π —ç—Å—ç—Ö–∏–π–≥ —É—Ä—å–¥—á–∏–ª–∞–Ω —Ç–∞–∞–º–∞–≥–ª–∞—Ö –∑–∞–≥–≤–∞—Ä –±“Ø—Ç—ç—ç—Ö, –º”©–Ω –∑“Ø—Ä—Ö–Ω–∏–π ”©–≤—á–Ω–∏–π —ç—Ä—Å–¥—ç–ª–∏–π–≥ –Ω—ç–º—ç–≥–¥“Ø“Ø–ª–¥—ç–≥ —Ö“Ø—á–∏–Ω –∑“Ø–π–ª—Å–∏–π–≥ –∏–ª—Ä“Ø“Ø–ª—ç—Ö.\n",
    "\n",
    "## üßæ –¢“Ø“Ø—Ö—ç–Ω –º—ç–¥—ç—ç–ª—ç–ª  \n",
    "–≠–Ω—ç—Ö“Ø“Ø –¥–∞—Ç–∞—Å–µ—Ç –Ω—å UCI-–∏–π–Ω –ú–∞—à–∏–Ω –°—É—Ä–≥–∞–ª—Ç—ã–Ω –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–¥ –±–∞–≥—Ç–¥–∞–≥ –±”©–≥”©”©–¥ –ö–ª–∏–≤–ª–µ–Ω–¥ –∑—ç—Ä—ç–≥ —Ö—ç–¥ —Ö—ç–¥—ç–Ω –∞–Ω–∞–≥–∞–∞—Ö —É—Ö–∞–∞–Ω—ã –±–∞–π–≥—É—É–ª–ª–∞–≥—ã–Ω ”©–≥”©–≥–¥”©–ª –¥—ç—ç—Ä “Ø–Ω–¥—ç—Å–ª—ç—Å—ç–Ω. Dataset –Ω—å –∑“Ø—Ä—Ö–Ω–∏–π ”©–≤—á–Ω–∏–π —ç–º–Ω—ç–ª –∑“Ø–π–Ω –æ–Ω–æ—à–∏–ª–≥–æ–æ–Ω–¥ —Ö—ç—Ä—ç–≥–ª—ç–≥–¥–¥—ç–≥ 13 “Ø–Ω–¥—Å—ç–Ω —à–∏–Ω–∂ —á–∞–Ω–∞—Ä—ã–≥ (features) –∞–≥—É—É–ª—Å–∞–Ω. –≠—Ü—Å–∏–π–Ω –∑–æ—Ä–∏–ª—Ç–æ—Ç —Ö—É–≤—å—Å–∞–≥—á –Ω—å —Ç—É—Ö–∞–π–Ω —Ö“Ø–Ω –∑“Ø—Ä—Ö–Ω–∏–π ”©–≤—á—Ç—ç–π —ç—Å—ç—Ö–∏–π–≥ –∏–ª—Ç–≥—ç–Ω—ç (`1 = ”©–≤—á—Ç—ç–π`, `0 = —ç—Ä“Ø“Ø–ª`).\n",
    "\n",
    "## üß† –ê—Å—É—É–¥–ª—ã–Ω —Ç”©—Ä”©–ª  \n",
    "–≠–Ω—ç –±–æ–ª **2 –∞–Ω–≥–∏–ª–ª—ã–Ω (binary classification)** –∞—Å—É—É–¥–∞–ª –±”©–≥”©”©–¥:\n",
    "- **1 ‚Üí –ó“Ø—Ä—Ö–Ω–∏–π ”©–≤—á—Ç—ç–π**\n",
    "- **0 ‚Üí –≠—Ä“Ø“Ø–ª**\n",
    "\n",
    "## ‚ùì –•–∞—Ä–∏—É–ª–∞—Ö —ë—Å—Ç–æ–π –≥–æ–ª –∞—Å—É—É–ª—Ç—É—É–¥:\n",
    "1. –Ø–º–∞—Ä —à–∏–Ω–∂“Ø“Ø–¥ –∑“Ø—Ä—Ö–Ω–∏–π ”©–≤—á–Ω–∏–π –º–∞–≥–∞–¥–ª–∞–ª–¥ —Ö–∞–º–≥–∏–π–Ω –∏—Ö –Ω”©–ª”©”©–ª–∂ –±–∞–π–Ω–∞ –≤—ç?\n",
    "2. –ù–∞—Å, —Ö“Ø–π—Å, —Ü—É—Å–Ω—ã –¥–∞—Ä–∞–ª—Ç, —Ö–æ–ª–µ—Å—Ç—Ä–æ–ª –∑—ç—Ä—ç–≥ —Ö—É–≤—å—Å–∞–≥—á–∏–¥ —Ö—ç—Ä –∏—Ö –Ω”©–ª”©”©–ª–∂ –±–∞–π–Ω–∞?\n",
    "3. –°—Ç—Ä–µ—Å—Å —Ç–µ—Å—Ç–∏–π–Ω “Ø—Ä –¥“Ø–Ω, ECG “Ø–∑“Ø“Ø–ª—ç–ª—Ç, –¥–∞—Å–≥–∞–ª—ã–Ω –¥–∞—Ä–∞–∞—Ö –∞–Ω–≥–∏–Ω–∞ –∑—ç—Ä—ç–≥ –Ω—å –∑“Ø—Ä—Ö–Ω–∏–π ”©–≤—á–∏–Ω—Ç—ç–π —Ö–∞–º–∞–∞—Ä–∞–ª—Ç–∞–π —é—É?\n",
    "4. –ó–∞–≥–≤–∞—Ä—ã–Ω “Ø—Ä –¥“Ø–Ω–¥ “Ø–Ω–¥—ç—Å–ª—ç–Ω —ç—Ä—Å–¥—ç–ª ”©–Ω–¥”©—Ä—Ç—ç–π —Ö“Ø–º“Ø“Ø—Å–∏–π–≥ —è–ª–≥–∞–∂ –±–æ–ª–æ—Ö —É—É?\n",
    "5. –ó”©–≤ —Ç–∞–∞–º–∞–≥–ª–∞–ª ”©–≥”©—Ö –Ω–∞–π–¥–≤–∞—Ä—Ç–∞–π –∑–∞–≥–≤–∞—Ä –±“Ø—Ç—ç—ç–∂ —á–∞–¥–∞—Ö —É—É?\n",
    "\n",
    "## üìè –ê–º–∂–∏–ª—Ç—ã–≥ —Ö—ç–º–∂–∏—Ö —à–∞–ª–≥—É—É—Ä:\n",
    "- –¢–µ—Å—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥—ç—ç—Ä—Ö **–∑–∞–≥–≤–∞—Ä—ã–Ω “Ø–Ω—ç–ª–≥—ç—ç–Ω–∏–π “Ø–∑“Ø“Ø–ª—ç–ª—Ç“Ø“Ø–¥**:  \n",
    "  - Accuracy  \n",
    "  - Precision  \n",
    "  - Recall  \n",
    "  - F1-score  \n",
    "  - ROC AUC\n",
    "\n",
    "- –ó–∞–≥–≤–∞—Ä—ã–Ω **“Ø—Ä –¥“Ø–Ω–≥ —Ç–∞–π–ª–±–∞—Ä–ª–∞—Ö**, —à–∏–Ω–∂“Ø“Ø–¥–∏–π–Ω —á—É—Ö–ª—ã–≥ —Ç–æ–¥–æ—Ä—Ö–æ–π–ª–æ—Ö —á–∞–¥–≤–∞—Ä\n",
    "\n",
    "## üöß –•—è–∑–≥–∞–∞—Ä–ª–∞–ª—Ç, –∞–Ω—Ö–∞–∞—Ä–∞—Ö –∑“Ø–π–ª—Å:\n",
    "- Dataset –Ω—å 300 –æ—Ä—á–∏–º –±–∏—á–ª—ç–≥—Ç—ç–π ‚Äî —Ö–∞—Ä—å—Ü–∞–Ω–≥—É–π –±–∞–≥–∞ —Ö—ç–º–∂—ç—ç—Ç—ç–π\n",
    "- –ó–∞—Ä–∏–º —É—Ç–≥—É—É–¥ –¥—É—Ç—É—É –±–∞–π–∂ –±–æ–ª–Ω–æ (`NaN`, `?`)\n",
    "- –ó–∞–≥–≤–∞—Ä—ã–Ω overfitting —Ö–∏–π—Ö—ç—ç—Å —Å—ç—Ä–≥–∏–π–ª—ç—Ö —Ö—ç—Ä—ç–≥—Ç—ç–π\n",
    "- –ú–µ–¥–∏—Ü–∏–Ω –æ—Ä—á–Ω—ã–≥ –æ–π–ª–≥–æ–º–∂—Ç–æ–π —Ç–∞–π–ª–±–∞—Ä–ª–∞–∂ –∞—à–∏–≥–ª–∞—Ö —Ö—ç—Ä—ç–≥—Ç—ç–π\n",
    "\n",
    "## üì¶ –®–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π “Ø—Ä –¥“Ø–Ω:\n",
    "1. –¶—ç–≤—ç—Ä–ª—ç–≥–¥—Å—ç–Ω, –±–æ–ª–æ–≤—Å—Ä—É—É–ª—Å–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç\n",
    "2. **Exploratory Data Analysis**: –≥—Ä–∞—Ñ–∏–∫, —Ö–∞–º–∞–∞—Ä–ª—ã–Ω –º–∞—Ç—Ä–∏—Ü, —Ç–∞—Ä—Ö–∞–ª—Ç\n",
    "3. Feature Engineering: –∫–∞—Ç–µ–≥–æ—Ä–∏—Ç —à–∏–Ω–∂“Ø“Ø–¥ –∫–æ–¥–ª–æ—Ö, scale —Ö–∏–π—Ö\n",
    "4. –°—É—Ä–≥–∞—Å–∞–Ω ML –∑–∞–≥–≤–∞—Ä—É—É–¥ (Logistic, Decision Tree, SVM, Neural Net)\n",
    "5. –ó–∞–≥–≤–∞—Ä—ã–Ω “Ø—Ä –¥“Ø–Ω–≥ —Ç–∞–π–ª–±–∞—Ä–ª–∞—Å–∞–Ω —Ç–∞–π–ª–∞–Ω: —è–º–∞—Ä —à–∏–Ω–∂“Ø“Ø–¥ —Ö–∞–º–≥–∏–π–Ω —á—É—Ö–∞–ª –±–∞–π—Å–∞–Ω, —è–º–∞—Ä –±“Ø–ª–≥“Ø“Ø–¥ —ç—Ä—Å–¥—ç–ª ”©–Ω–¥”©—Ä—Ç—ç–π –±–∞–π–Ω–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ”®–≥”©–≥–¥”©–ª –∞—á–∞–∞–ª–∂, EDA —Ö–∏–π—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0d7145c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú”©—Ä, –ë–∞–≥–∞–Ω—ã–Ω —Ç–æ–æ: (920, 16)\n",
      "\n",
      "–≠—Ö–Ω–∏–π –º”©—Ä–Ω“Ø“Ø–¥:\n",
      "   id  age     sex    dataset               cp  trestbps   chol    fbs  \\\n",
      "0   1   63    Male  Cleveland   typical angina     145.0  233.0   True   \n",
      "1   2   67    Male  Cleveland     asymptomatic     160.0  286.0  False   \n",
      "2   3   67    Male  Cleveland     asymptomatic     120.0  229.0  False   \n",
      "3   4   37    Male  Cleveland      non-anginal     130.0  250.0  False   \n",
      "4   5   41  Female  Cleveland  atypical angina     130.0  204.0  False   \n",
      "\n",
      "          restecg  thalch  exang  oldpeak        slope   ca  \\\n",
      "0  lv hypertrophy   150.0  False      2.3  downsloping  0.0   \n",
      "1  lv hypertrophy   108.0   True      1.5         flat  3.0   \n",
      "2  lv hypertrophy   129.0   True      2.6         flat  2.0   \n",
      "3          normal   187.0  False      3.5  downsloping  0.0   \n",
      "4  lv hypertrophy   172.0  False      1.4    upsloping  0.0   \n",
      "\n",
      "                thal  num  \n",
      "0       fixed defect    0  \n",
      "1             normal    2  \n",
      "2  reversable defect    1  \n",
      "3             normal    0  \n",
      "4             normal    0  \n",
      "\n",
      "–î–∞—Ç–∞–Ω—ã –µ—Ä”©–Ω—Ö–∏–π –º—ç–¥—ç—ç–ª—ç–ª:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 920 entries, 0 to 919\n",
      "Data columns (total 16 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   id        920 non-null    int64  \n",
      " 1   age       920 non-null    int64  \n",
      " 2   sex       920 non-null    object \n",
      " 3   dataset   920 non-null    object \n",
      " 4   cp        920 non-null    object \n",
      " 5   trestbps  861 non-null    float64\n",
      " 6   chol      890 non-null    float64\n",
      " 7   fbs       830 non-null    object \n",
      " 8   restecg   918 non-null    object \n",
      " 9   thalch    865 non-null    float64\n",
      " 10  exang     865 non-null    object \n",
      " 11  oldpeak   858 non-null    float64\n",
      " 12  slope     611 non-null    object \n",
      " 13  ca        309 non-null    float64\n",
      " 14  thal      434 non-null    object \n",
      " 15  num       920 non-null    int64  \n",
      "dtypes: float64(5), int64(3), object(8)\n",
      "memory usage: 115.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/raw/heart_disease_uci.csv')\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "print(\"–ú”©—Ä, –ë–∞–≥–∞–Ω—ã–Ω —Ç–æ–æ:\", df.shape)\n",
    "print(\"\\n–≠—Ö–Ω–∏–π –º”©—Ä–Ω“Ø“Ø–¥:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n–î–∞—Ç–∞–Ω—ã –µ—Ä”©–Ω—Ö–∏–π –º—ç–¥—ç—ç–ª—ç–ª:\")\n",
    "print(df.info())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "4bea108d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫ “Ø–∑“Ø“Ø–ª—ç–ª—Ç:\n",
      "               id         age    trestbps        chol      thalch     oldpeak  \\\n",
      "count  920.000000  920.000000  861.000000  890.000000  865.000000  858.000000   \n",
      "mean   460.500000   53.510870  132.132404  199.130337  137.545665    0.878788   \n",
      "std    265.725422    9.424685   19.066070  110.780810   25.926276    1.091226   \n",
      "min      1.000000   28.000000    0.000000    0.000000   60.000000   -2.600000   \n",
      "25%    230.750000   47.000000  120.000000  175.000000  120.000000    0.000000   \n",
      "50%    460.500000   54.000000  130.000000  223.000000  140.000000    0.500000   \n",
      "75%    690.250000   60.000000  140.000000  268.000000  157.000000    1.500000   \n",
      "max    920.000000   77.000000  200.000000  603.000000  202.000000    6.200000   \n",
      "\n",
      "               ca         num  \n",
      "count  309.000000  920.000000  \n",
      "mean     0.676375    0.995652  \n",
      "std      0.935653    1.142693  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    0.000000  \n",
      "50%      0.000000    1.000000  \n",
      "75%      1.000000    2.000000  \n",
      "max      3.000000    4.000000  \n",
      "\n",
      "–ë–∞–≥–∞–Ω–∞–¥ –¥—É—Ç—É—É —É—Ç–≥–∞ –±–∞–π–≥–∞–∞ —ç—Å—ç—Ö:\n",
      "id            0\n",
      "age           0\n",
      "sex           0\n",
      "dataset       0\n",
      "cp            0\n",
      "trestbps     59\n",
      "chol         30\n",
      "fbs          90\n",
      "restecg       2\n",
      "thalch       55\n",
      "exang        55\n",
      "oldpeak      62\n",
      "slope       309\n",
      "ca          611\n",
      "thal        486\n",
      "num           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫ “Ø–∑“Ø“Ø–ª—ç–ª—Ç:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\n–ë–∞–≥–∞–Ω–∞–¥ –¥—É—Ç—É—É —É—Ç–≥–∞ –±–∞–π–≥–∞–∞ —ç—Å—ç—Ö:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing —Ö–∏–π—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_25336\\1905211042.py:22: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(mode[0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# üéØ Target –±–∞–≥–∞–Ω–∞ “Ø“Ø—Å–≥—ç—Ö\n",
    "df[\"target\"] = (df[\"num\"] > 0).astype(int)\n",
    "\n",
    "# üßπ num/id –±–∞–≥–∞–Ω—É—É–¥—ã–≥ —Ö–∞—Å–∞—Ö ‚Äî –∞–ª—å –∞–ª—å –Ω—å —ç—Ä—Ç—Ö—ç–Ω —Ö–∞—Å–∞–≥–¥–∞—Ö —ë—Å—Ç–æ–π\n",
    "df.drop([\"num\", \"id\"], axis=1, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# üß† –ë–∞–≥–∞–Ω–∞ —Ç”©—Ä–ª”©”©—Ä –∞–Ω–≥–∏–ª–∞—Ö\n",
    "numerical_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n",
    "\n",
    "# üéØ Target-–≥ numeric list-–æ–æ—Å —Ö–∞—Å–∞—Ö\n",
    "numerical_cols = [col for col in numerical_cols if col != \"target\"]\n",
    "\n",
    "# üîÅ Numerical –±–∞–≥–∞–Ω—É—É–¥—ã–≥ median-—Ä –Ω”©—Ö”©—Ö\n",
    "for col in numerical_cols:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# üîÅ Categorical –±–∞–≥–∞–Ω—É—É–¥—ã–≥ mode-—Ä –Ω”©—Ö”©—Ö\n",
    "for col in categorical_cols:\n",
    "    mode = df[col].mode()\n",
    "    if not mode.empty:\n",
    "        df[col] = df[col].fillna(mode[0])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0cb6db9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–≠—Ö–Ω–∏–π –º”©—Ä–Ω“Ø“Ø–¥:\n",
      "   age     sex    dataset               cp  trestbps   chol    fbs  \\\n",
      "0   63    Male  Cleveland   typical angina     145.0  233.0   True   \n",
      "1   67    Male  Cleveland     asymptomatic     160.0  286.0  False   \n",
      "2   67    Male  Cleveland     asymptomatic     120.0  229.0  False   \n",
      "3   37    Male  Cleveland      non-anginal     130.0  250.0  False   \n",
      "4   41  Female  Cleveland  atypical angina     130.0  204.0  False   \n",
      "\n",
      "          restecg  thalch  exang  oldpeak        slope   ca  \\\n",
      "0  lv hypertrophy   150.0  False      2.3  downsloping  0.0   \n",
      "1  lv hypertrophy   108.0   True      1.5         flat  3.0   \n",
      "2  lv hypertrophy   129.0   True      2.6         flat  2.0   \n",
      "3          normal   187.0  False      3.5  downsloping  0.0   \n",
      "4  lv hypertrophy   172.0  False      1.4    upsloping  0.0   \n",
      "\n",
      "                thal  target  \n",
      "0       fixed defect       0  \n",
      "1             normal       1  \n",
      "2  reversable defect       1  \n",
      "3             normal       0  \n",
      "4             normal       0  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n–≠—Ö–Ω–∏–π –º”©—Ä–Ω“Ø“Ø–¥:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "1fc78ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–ë–∞–≥–∞–Ω–∞–¥ –¥—É—Ç—É—É —É—Ç–≥–∞ –±–∞–π–≥–∞–∞ —ç—Å—ç—Ö:\n",
      "age         0\n",
      "sex         0\n",
      "dataset     0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalch      0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "target      0\n",
      "dtype: int64\n",
      "–ú”©—Ä, –ë–∞–≥–∞–Ω—ã–Ω —Ç–æ–æ: (920, 15)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n–ë–∞–≥–∞–Ω–∞–¥ –¥—É—Ç—É—É —É—Ç–≥–∞ –±–∞–π–≥–∞–∞ —ç—Å—ç—Ö:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"–ú”©—Ä, –ë–∞–≥–∞–Ω—ã–Ω —Ç–æ–æ:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "654bbafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "df.to_csv('../data/processed/heart_disease_uci_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae27a5a",
   "metadata": {},
   "source": [
    "Data visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "3ecfa494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# –ó—É—Ä–≥–∏–π–≥ —Ñ–æ–ª–¥–µ—Ä—Ç —Ö–∞–¥–≥–∞–ª–∞—Ö \n",
    "output_dir = \"../outputs/figures\"\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. ”®–≤—á—Ç—ç–π —ç—Å—ç—Ö —Ö—É–≤—å\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='target', data=df)\n",
    "plt.title('Heart Disease Presence (0 = Healthy, 1+ = Diseased)')\n",
    "plt.savefig(f'{output_dir}/disease_count.png')\n",
    "plt.close()\n",
    "\n",
    "# 2. ”®–≤—á–ª”©–ª–∏–π–Ω –±–∞–π–¥–∞–ª –Ω–∞—Å–Ω—ã –±“Ø–ª–≥—ç—ç—Ä\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='age', hue='target', bins=20, kde=True)\n",
    "plt.title('Age Distribution by Heart Disease')\n",
    "plt.savefig(f'{output_dir}/age_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# 3. ”®–≤—á–ª”©–ª–∏–π–Ω –±–∞–π–¥–∞–ª —Ö“Ø–π—Å—ç—ç—Ä\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='sex', hue='target', data=df)\n",
    "plt.title('Heart Disease by Gender (1 = Male, 0 = Female)')\n",
    "plt.savefig(f'{output_dir}/disease_by_gender.png')\n",
    "plt.close()\n",
    "\n",
    "# 4. Chest pain type-—Ä ”©–≤—á–ª”©–ª\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='cp', hue='target', data=df)\n",
    "plt.title('Heart Disease by Chest Pain Type')\n",
    "plt.savefig(f'{output_dir}/disease_by_cp.png')\n",
    "plt.close()\n",
    "\n",
    "# 5. Thal (Thalassemia) vs Disease\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='thal', hue='target', data=df)\n",
    "plt.title('Heart Disease by Thalassemia Type')\n",
    "plt.savefig(f'{output_dir}/disease_by_thal.png')\n",
    "plt.close()\n",
    "\n",
    "# 6. ST depression (oldpeak) —Ç–∞—Ä—Ö–∞–ª—Ç\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(data=df, x='oldpeak', hue='target', bins=30, kde=True)\n",
    "plt.title('Oldpeak (ST Depression) by Heart Disease')\n",
    "plt.savefig(f'{output_dir}/oldpeak_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# 7. Maximum heart rate (thalach) —Ç–∞—Ä—Ö–∞–ª—Ç\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(data=df, x='thalch', hue='target', bins=30, kde=True)\n",
    "plt.title('Max Heart Rate (Thalach) by Disease')\n",
    "plt.savefig(f'{output_dir}/thalach_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# 8. Correlation matrix (—Ç–æ–æ–Ω —Ö—É–≤—å—Å–∞–≥—á–∏–¥)\n",
    "numerical_cols = ['age', 'trestbps', 'chol', 'thalch', 'oldpeak', 'ca', 'target']\n",
    "corr_matrix = df[numerical_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Numeric Features')\n",
    "plt.savefig(f'{output_dir}/correlation_matrix.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c894b7a1",
   "metadata": {},
   "source": [
    "## 3. –ó–∞–≥–≤–∞—Ä —Å—É—Ä–≥–∞–ª—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e987c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LogisticRegression', 0.842391304347826, {'classifier__C': 1.0})"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Re-execute after kernel reset: prepare environment again\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "os.makedirs(\"../outputs/figures\", exist_ok=True)\n",
    "os.makedirs(\"../outputs/reports\", exist_ok=True)\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "# Load the uploaded dataset\n",
    "df = pd.read_csv(\"../data/processed/heart_disease_uci_cleaned.csv\")\n",
    "\n",
    "# Prepare data\n",
    "\n",
    "\n",
    "numerical_features = df.select_dtypes(include=[\"int64\", \"float64\"]).drop(columns=[\"target\"]).columns.tolist()\n",
    "categorical_features = df.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n",
    "\n",
    "# Preprocessing\n",
    "num_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "cat_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_transformer, numerical_features),\n",
    "    (\"cat\", cat_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Models\n",
    "RANDOM_STATE = 42\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    \"SVC\": SVC(probability=True, random_state=RANDOM_STATE),\n",
    "    \"MLPClassifier\": MLPClassifier(random_state=RANDOM_STATE, max_iter=1000)\n",
    "}\n",
    "\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "pipelines = {}\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", model)\n",
    "    ])\n",
    "    scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "    pipelines[name] = pipe\n",
    "    cv_results[name] = {\n",
    "        \"mean_accuracy\": scores.mean(),\n",
    "        \"std_accuracy\": scores.std()\n",
    "    }\n",
    "param_grid = {}\n",
    "best_model_name = max(cv_results.items(), key=lambda x: x[1][\"mean_accuracy\"])[0]\n",
    "best_pipeline = pipelines[best_model_name]\n",
    "grid_search = GridSearchCV(best_pipeline, param_grid, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "# Evaluate on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Define grid\n",
    "\n",
    "if best_model_name == \"RandomForest\":\n",
    "    param_grid = {\n",
    "        \"classifier__n_estimators\": [100, 200],\n",
    "        \"classifier__max_depth\": [None, 5, 10],\n",
    "    }\n",
    "elif best_model_name == \"GradientBoosting\":\n",
    "    param_grid = {\n",
    "        \"classifier__n_estimators\": [100, 200],\n",
    "        \"classifier__learning_rate\": [0.01, 0.1],\n",
    "        \"classifier__max_depth\": [3, 5]\n",
    "    }\n",
    "elif best_model_name == \"LogisticRegression\":\n",
    "    param_grid = {\n",
    "        \"classifier__C\": [0.01, 0.1, 1.0, 10.0]\n",
    "    }\n",
    "elif best_model_name == \"DecisionTree\":\n",
    "    param_grid = {\n",
    "        \"classifier__max_depth\": [None, 5, 10, 15],\n",
    "        \"classifier__min_samples_split\": [2, 5, 10]\n",
    "    }\n",
    "elif best_model_name == \"SVC\":\n",
    "    param_grid = {\n",
    "        \"classifier__C\": [0.1, 1, 10],\n",
    "        \"classifier__kernel\": ['linear', 'rbf']\n",
    "    }\n",
    "elif best_model_name == \"MLPClassifier\":\n",
    "    param_grid = {\n",
    "        \"classifier__hidden_layer_sizes\": [(50,), (100,), (50, 50)],\n",
    "        \"classifier__alpha\": [0.0001, 0.001]\n",
    "    }\n",
    "    \n",
    "    \n",
    "report_path = \"../outputs/reports/classification_report.txt\"\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(\"Accuracy: {:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\n",
    "    f.write(classification_report(y_test, y_pred))\n",
    "    f.write(\"\\nConfusion Matrix:\\n\")\n",
    "    f.write(np.array2string(confusion_matrix(y_test, y_pred)))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(best_model, \"../models/heart_disease_best_model.pkl\")\n",
    "\n",
    "# Feature importance\n",
    "if hasattr(best_model[\"classifier\"], \"feature_importances_\"):\n",
    "    if hasattr(best_model[\"preprocessor\"], \"get_feature_names_out\"):\n",
    "        feature_names = best_model[\"preprocessor\"].get_feature_names_out()\n",
    "    else:\n",
    "        feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "    \n",
    "    importance = best_model[\"classifier\"].feature_importances_\n",
    "    fi_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importance})\n",
    "    fi_df.sort_values(\"Importance\", ascending=False, inplace=True)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x=\"Importance\", y=\"Feature\", data=fi_df.head(15))\n",
    "    plt.title(\"Top Feature Importances\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../outputs/figures/feature_importance.png\")\n",
    "    plt.close()\n",
    "\n",
    "# ROC Curve\n",
    "if hasattr(best_model[\"classifier\"], \"predict_proba\"):\n",
    "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\", lw=2)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"../outputs/figures/roc_curve.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Summary\n",
    "summary_txt = f\"\"\"Best Model: {best_model_name}\n",
    "Test Accuracy: {accuracy_score(y_test, y_pred):.4f}\n",
    "Best Parameters: {grid_search.best_params_}\n",
    "\"\"\"\n",
    "\n",
    "with open(\"../outputs/reports/summary.txt\", \"w\") as f:\n",
    "    f.write(summary_txt)\n",
    "\n",
    "\"‚úÖ –ú–æ–¥–µ–ª –∞–º–∂–∏–ª—Ç—Ç–∞–π —Å—É—Ä–≥–∞–≤. –ó–∞–≥–≤–∞—Ä –±–æ–ª–æ–Ω —Ç–∞–π–ª–∞–Ω–≥—É—É–¥ —Ö–∞–¥–≥–∞–ª–∞–≥–¥—Å–∞–Ω!\"\n",
    "\n",
    "grid_search = GridSearchCV(best_pipeline, param_grid, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Final evaluation\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "best_model_name, accuracy, grid_search.best_params_\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
